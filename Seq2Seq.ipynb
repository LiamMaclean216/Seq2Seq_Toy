{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from masked_cross_entropy import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_sequences(length_from, length_to,\n",
    "                     vocab_lower, vocab_upper,\n",
    "                     batch_size):\n",
    "    \"\"\" Generates batches of random integer sequences,\n",
    "        sequence length in [length_from, length_to],\n",
    "        vocabulary in [vocab_lower, vocab_upper]\n",
    "    \"\"\"\n",
    "    if length_from > length_to:\n",
    "            raise ValueError('length_from > length_to')\n",
    "\n",
    "    def random_length():\n",
    "        if length_from == length_to:\n",
    "            return length_from\n",
    "        return np.random.randint(length_from, length_to + 1)\n",
    "    \n",
    "    while True:\n",
    " \n",
    "        \n",
    "        padded = np.zeros([batch_size,length_to])\n",
    "        seq_lengths = np.zeros([batch_size])\n",
    "        for i in range(batch_size):\n",
    "            rand = np.random.randint(low=vocab_lower,\n",
    "                              high=vocab_upper,\n",
    "                              size=random_length()).tolist()\n",
    "            seq_lengths[i] = len(rand)\n",
    "            padded[i,0:len(rand)] = rand\n",
    "            \n",
    "       \n",
    "        concat = np.zeros([batch_size,padded.shape[1]+1])\n",
    "        concat[:,0] = seq_lengths\n",
    "        concat[:,1:concat.shape[1]] = padded\n",
    "        \n",
    "        yield concat[:,1:],concat[:,0].astype(int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_layers=1, dropout=0):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.gru = nn.GRU(input_size, hidden_size, n_layers, dropout=self.dropout, bidirectional=True)\n",
    "        self.hidden = None\n",
    "    def forward(self, input_seqs, input_lengths):\n",
    "        # Note: we run this all at once (over multiple batches of multiple sequences)\n",
    "        #embedded = self.embedding(input_seqs)\n",
    "        input_seqs = input_seqs.unsqueeze(-1).type(torch.FloatTensor)\n",
    "        packed = torch.nn.utils.rnn.pack_padded_sequence(input_seqs, input_lengths)\n",
    "        outputs, self.hidden = self.gru(packed, self.hidden)\n",
    "        outputs, output_lengths = torch.nn.utils.rnn.pad_packed_sequence(outputs) # unpack (back to padded)\n",
    "        outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:] # Sum bidirectional outputs\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "    \n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self,output_size,hidden_size,max_seq_length,enc_hidden_size,n_layers=1):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.enc_hidden_size = enc_hidden_size\n",
    "\n",
    "        self.gru = nn.GRU(enc_hidden_size, hidden_size, n_layers, bidirectional=True)\n",
    "        self.hidden = None\n",
    "       \n",
    "        self.concat = nn.Linear(hidden_size*2,hidden_size)\n",
    "        self.out = nn.Linear(hidden_size,output_size)\n",
    "        \n",
    "        \n",
    "    def forward(self, encoder_outputs):\n",
    "        rnn_in = encoder_outputs[-1].unsqueeze(0)\n",
    "           \n",
    "        outputs, self.hidden = self.gru(rnn_in, self.hidden)\n",
    "        outputs = self.concat(outputs)\n",
    "        out = self.out(outputs)\n",
    "\n",
    "        return out\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss : 4.190133734941482  Real : [1. 1. 5.]  Pred : [0.16370735 0.15192667 0.15083903]\n",
      "Loss : 1.9762819989522298  Real : [1. 8. 5.]  Pred : [3.77183771 4.92165184 5.20417118]\n",
      "Loss : 1.8544980382919312  Real : [7. 5. 1.]  Pred : [2.46427679 2.66099334 2.5252831 ]\n",
      "Loss : 1.1701394279797872  Real : [8. 7. 8.]  Pred : [6.17251587 7.22404528 7.39565897]\n",
      "Loss : 1.577463877995809  Real : [8. 5. 8.]  Pred : [5.82043695 7.03967285 7.2842598 ]\n",
      "Loss : 0.9567384751637776  Real : [4. 1. 2.]  Pred : [2.88248682 2.86798477 1.93761647]\n",
      "Loss : 0.994472173055013  Real : [8. 3. 7.]  Pred : [7.14508724 7.1848278  6.50663805]\n",
      "Loss : 0.8063480011622111  Real : [1. 6. 8.]  Pred : [2.16676426 4.9029665  7.44788265]\n",
      "Loss : 0.7817811330159505  Real : [4. 2. 3.]  Pred : [3.70629096 3.52619076 2.85771084]\n",
      "Loss : 0.8273346964518229  Real : [8. 2. 2.]  Pred : [7.67947817 4.16674995 1.7464664 ]\n",
      "Loss : 0.8382234287261963  Real : [3. 2. 8.]  Pred : [2.95862103 5.84107733 7.94989634]\n",
      "Loss : 0.673198504447937  Real : [4. 8. 8.]  Pred : [3.963346   5.82367277 7.85669661]\n",
      "Loss : 0.5984942388534545  Real : [3. 8. 4.]  Pred : [3.18934155 4.50982475 3.98155618]\n",
      "Loss : 0.7396027723948161  Real : [4. 7. 5.]  Pred : [4.03835487 4.88538885 5.00496912]\n",
      "Loss : 0.6935588550567627  Real : [8. 2. 2.]  Pred : [8.0779686  3.91705394 2.01808238]\n",
      "Loss : 0.4681324299176534  Real : [5. 6. 6.]  Pred : [4.99113941 5.06226301 5.98103952]\n",
      "Loss : 0.5114907439549764  Real : [8. 3. 2.]  Pred : [8.18806744 3.70095158 2.11635184]\n",
      "Loss : 0.22011219342549643  Real : [1. 2. 3.]  Pred : [0.89472938 2.48596907 2.64278889]\n",
      "Loss : 0.18140445073445638  Real : [6. 8. 1.]  Pred : [5.8255024  7.78829861 1.12612569]\n",
      "Loss : 0.07799788157145182  Real : [3. 7. 4.]  Pred : [3.01222539 7.02940941 4.03143311]\n",
      "Loss : 0.07250036398569742  Real : [7. 3. 5.]  Pred : [6.97770834 2.97626233 5.02620077]\n",
      "Loss : 0.06177874485651652  Real : [1. 1. 7.]  Pred : [0.98896635 1.37391245 7.09209061]\n",
      "Loss : 0.07556112130482992  Real : [4. 6. 4.]  Pred : [3.9583087  6.16592455 4.02433538]\n",
      "Loss : 0.06005836566289266  Real : [2. 6. 6.]  Pred : [1.99180365 6.07551384 6.0375824 ]\n",
      "Loss : 0.08033358256022136  Real : [1. 7. 4.]  Pred : [0.9770633  6.6897459  4.03597307]\n",
      "Loss : 0.06993547439575196  Real : [6. 6. 1.]  Pred : [6.07078505 6.10695839 0.93122327]\n",
      "Loss : 0.054822381337483725  Real : [7. 6. 4.]  Pred : [6.96930027 6.07504845 3.97931123]\n",
      "Loss : 0.05205809752146403  Real : [1. 1. 8.]  Pred : [0.97570062 1.22643864 7.90178776]\n",
      "Loss : 0.038800089359283446  Real : [7. 1. 1.]  Pred : [6.98637056 1.05559003 1.01520264]\n",
      "Loss : 0.03981788953145345  Real : [5. 7. 2.]  Pred : [5.00342989 7.13589907 1.98249125]\n",
      "Loss : 0.03164236783981323  Real : [2. 1. 8.]  Pred : [1.9838469  1.11771131 7.99321985]\n",
      "Loss : 0.045020237763722736  Real : [8. 5. 8.]  Pred : [8.01654243 4.89513111 8.03697777]\n",
      "Loss : 0.03927796681722005  Real : [2. 3. 7.]  Pred : [2.03394938 3.03571272 7.03854561]\n",
      "Loss : 0.04548649390538533  Real : [3. 5. 1.]  Pred : [3.00022411 5.05643845 0.96178466]\n",
      "Loss : 0.05635842482248942  Real : [3. 1. 8.]  Pred : [3.01964068 0.89207131 7.95666599]\n",
      "Loss : 0.02868951956431071  Real : [2. 5. 7.]  Pred : [1.99807417 4.94315958 6.99305487]\n",
      "Loss : 0.04799678564071655  Real : [1. 4. 7.]  Pred : [0.9938938  3.95183468 6.94362736]\n",
      "Loss : 0.022705828348795573  Real : [4. 1. 5.]  Pred : [3.96552038 1.01210988 5.00198698]\n",
      "Loss : 0.03149837891260783  Real : [4. 5. 1.]  Pred : [3.96537185 5.06210279 0.95422989]\n",
      "Loss : 0.031302235921223956  Real : [3. 7. 7.]  Pred : [2.99923348 7.04589844 7.04734182]\n",
      "Loss : 0.029750359058380128  Real : [6. 7. 2.]  Pred : [5.9761982  7.0506568  2.01513743]\n",
      "Loss : 0.022281254132588704  Real : [4. 6. 5.]  Pred : [3.98792839 6.00657177 4.99289989]\n",
      "Loss : 0.02534215529759725  Real : [6. 1. 8.]  Pred : [6.00608206 1.0843699  8.00814247]\n",
      "Loss : 0.02914077599843343  Real : [4. 4. 4.]  Pred : [3.97799897 4.02406931 4.00842094]\n",
      "Loss : 0.018757600784301758  Real : [2. 8. 5.]  Pred : [2.03775167 7.97861862 4.99907398]\n",
      "Loss : 0.02253275235493978  Real : [2. 6. 8.]  Pred : [1.96844006 6.05464792 8.03590012]\n",
      "Loss : 0.030264863967895506  Real : [1. 5. 8.]  Pred : [0.98835206 5.04183626 8.03490353]\n",
      "Loss : 0.027117412885030112  Real : [3. 4. 8.]  Pred : [2.96733594 3.88134813 7.95947504]\n",
      "Loss : 0.03108732541402181  Real : [8. 3. 3.]  Pred : [7.97595835 2.99342394 3.00045109]\n",
      "Loss : 0.023648170630137126  Real : [7. 6. 7.]  Pred : [6.96965599 6.01320839 6.97259378]\n",
      "Loss : 0.026400888760884603  Real : [6. 8. 2.]  Pred : [6.05003119 7.93497944 1.96878934]\n",
      "Loss : 0.035641098817189534  Real : [6. 7. 6.]  Pred : [6.0071454  6.95753288 5.96997929]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-557148253d41>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0menc_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mdec_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     87\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     88\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "max_seq_length = 3\n",
    "batch_size = 25    \n",
    "input_size =1\n",
    "\n",
    "hidden_size = 10\n",
    "enc_model = EncoderRNN(input_size,hidden_size)\n",
    "dec_model = DecoderRNN(input_size,hidden_size,max_seq_length,hidden_size)\n",
    "\n",
    "enc_optimizer = torch.optim.Adam(enc_model.parameters(), lr=0.001)\n",
    "dec_optimizer = torch.optim.Adam(dec_model.parameters(), lr=0.001)\n",
    "\n",
    "for i in range(10000):\n",
    "    enc_optimizer.zero_grad()\n",
    "    dec_optimizer.zero_grad()\n",
    "    d, s = next(random_sequences(max_seq_length, max_seq_length,\n",
    "                     1, 9, \n",
    "                     batch_size))\n",
    "\n",
    "    enc_out = enc_model(torch.from_numpy(d.T),torch.from_numpy(s))\n",
    "    \n",
    "    all_out = torch.from_numpy(np.zeros([max_seq_length,batch_size,input_size]))\n",
    "    #print(all_out.shape)\n",
    "    for j in range(max_seq_length):\n",
    "\n",
    "        dec_out = dec_model(enc_out)\n",
    "        all_out[j] = dec_out\n",
    "    \n",
    "    d = torch.from_numpy(d).unsqueeze(-1).transpose(0,1)\n",
    "    \n",
    "    loss = torch.nn.L1Loss()(all_out,d)\n",
    "    \n",
    "    \n",
    "    loss.backward()\n",
    "    enc_optimizer.step()\n",
    "    dec_optimizer.step()\n",
    "    enc_model.hidden = None\n",
    "    dec_model.hidden = None\n",
    "    #d = d.squeeze(-1).detach().numpy().T[0]\n",
    "    #all_out = all_out.squeeze(-1).detach().numpy().T[0]\n",
    "    \n",
    "    d = d[:,0].squeeze(-1).detach().numpy()\n",
    "    \n",
    "    all_out = all_out[:,0].squeeze(-1).detach().numpy()\n",
    "    if i % 100 == 0:\n",
    "        print(\"Loss : {}  Real : {}  Pred : {}\".format(loss.detach().numpy(),d,all_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
